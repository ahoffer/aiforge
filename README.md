# Local AI Agent Stack

Self-hosted AI platform on Kubernetes with GPU-accelerated inference, web search, and vector storage.

## Vision
- The goal is agentic software development in which engineers supervise and guide AI agents rather than writing most code by hand.
- This marks a shift in how developers work, with more emphasis on orchestration, oversight, and the art/science of managing intelligent systems.

## Architecture

Four services in the `aiforge` namespace.

| Service | Role | Cluster DNS | NodePort |
|---------|------|-------------|----------|
| Ollama | GPU model serving | ollama:11434 | cluster-internal |
| SearXNG | Web search aggregation | searxng:8080 | :31080 |
| Qdrant | Vector embeddings for RAG | qdrant:6333 | :31333 |
| Proteus | LangGraph agent and transparent Ollama proxy | proteus:8000 | :31400 |

## Quick Start

Prerequisites: Kubernetes cluster with GPU support, NVIDIA device plugin, kubectl.

```bash
./install.sh
```

Verify with `./tests/test-stack.sh` and `./tests/test-services.sh`.

## Frontends

| Frontend | Description |
|----------|-------------|
| `goose.sh` | Terminal coding agent with MCP tools (web search, qdrant, files, git, shell) |
| `opencode.sh` | Terminal coding assistant with MCP tools and TUI (web search, qdrant, files, git, shell) |

Routing contract: `client -> proteus -> ollama`. Clients should never target Ollama directly.

Local models sometimes misinterpret intent. Use explicit phrasing like "analyze this code, do not edit any files" to avoid unintended edits.

## Proteus

FastAPI app in `images/proteus/` with two modes. The `/chat` and `/chat/stream` endpoints run a LangGraph workflow that calls Ollama with native tool calling and executes `web_search` server-side via SearXNG. The orchestrator node calls the model, and the tools node dispatches tool calls and loops back until the model produces a final answer. The `/v1/chat/completions` and `/v1/embeddings` endpoints are transparent proxies to Ollama, preserving tool_calls in the response so clients like opencode and goose can execute tools client-side via MCP.

Endpoint Surface (Native + OpenAI Compatibility):

| Surface | Method | Endpoint | Purpose / Notes |
|---------|--------|----------|-----------------|
| Native | POST | `/chat` | Server-side agent with tool execution via LangGraph. |
| Native | POST | `/chat/stream` | SSE streaming with node progress events. |
| Native | GET | `/health` | Dependency health check for Ollama and SearXNG. |
| Native | GET | `/health/live` | Kubernetes liveness probe. |
| Native | GET | `/health/ready` | Kubernetes readiness probe. |
| Native | GET | `/docs` | FastAPI OpenAPI docs UI. |
| OpenAI-compatible | POST | `/v1/chat/completions` | Transparent proxy to Ollama preserving tool_calls. |
| OpenAI-compatible | POST | `/v1/embeddings` | Embedding proxy to Ollama for MCP servers. |
| OpenAI-compatible | GET | `/v1/models` | Model list for client discovery. |
| OpenAI-compatible | GET | `/v1/models/{model_id}` | Single model retrieval for SDK validation. |

TODO: add thread-aware retrieval memory in the LangGraph loop. Ingestion is available via the `qdrant_index` MCP tool, but the server-side agent does not yet use conversation history for retrieval.

## Configuration

Launcher scripts source `config.env`. Override any variable at launch. Test scripts source `tests/test.env` for test-specific defaults.

Client config files generated by launcher scripts:
- Goose: `~/.config/goose/config.yaml` and `~/.config/goose/custom_providers/proteus.json`
- OpenCode: `~/.config/opencode/config.json`

| Variable | Default | Description |
|----------|---------|-------------|
| AGENT_BASE_MODEL | devstral:latest | Base model pulled from Ollama registry |
| AGENT_MODEL | ${AGENT_BASE_MODEL}-agent | Tuned alias with baked-in verbosity controls |
| AGENT_URL | http://bigfish:31400 | Proteus proxy URL |
| SEARXNG_URL | http://bigfish:31080 | SearXNG search API URL |
| QDRANT_URL | http://bigfish:31333 | Qdrant vector store URL |
| EMBEDDING_MODEL | nomic-embed-text | Model used for embeddings |

Model tuning parameters (temperature, top_p, max_tokens, repeat_penalty) live in the k8s ConfigMaps in `proteus.yaml` and are baked into the Ollama model alias at deploy time.

## Context Window Sizing

Devstral is 23.6B Q4_K_M with 40 layers, 8 KV heads, head dim 160.
Model weights and overhead consume ~14 GB of the GPU's 16 GB VRAM,
leaving ~2 GB for the KV cache that holds conversation context.
KV cache cost scales linearly with context length. Quantizing the
KV cache reduces per-token cost at the expense of minor precision loss.

| KV cache type | Cost per token | Max context in 2 GB | Recommended num_ctx | Tradeoff |
|---------------|----------------|---------------------|---------------------|----------|
| FP16 (default) | ~0.2 MiB | ~10,000 | 8192 | No quality loss, but tight for tool-heavy sessions with large system prompts |
| Q8_0 | ~0.1 MiB | ~20,000 | 16384 | Negligible quality loss, comfortable for most Goose and opencode conversations |
| Q4_0 | ~0.05 MiB | ~40,000 | 32768 | Some KV precision loss, generous context for long multi-turn sessions |

Set `OLLAMA_KV_CACHE_TYPE` in `k8s/ollama.yaml` and `AGENT_NUM_CTX` in
both `k8s/ollama.yaml` and `k8s/proteus.yaml`. Proteus injects num_ctx
into every Ollama request and reports the value on `GET /v1/models` so
clients can display accurate token counts.

## Testing

- `test-stack.sh` - SearXNG, Qdrant, and Proteus health checks
- `test-services.sh` - Qdrant CRUD and SearXNG search
- `test-agent.sh` - agent API including chat, streaming, and OpenAI-compatible endpoints
- `test-proxy-web-search-smoke.sh` - verifies server-side `web_search` executes for recency prompts on `/chat`
- `test-tool-calling.py` - 12 prompts across single-tool, no-tool, multi-tool categories (targets Ollama directly, not Proteus)
- `bench-ollama.sh` - generation speed, prompt eval, time to first token, tool call latency
- `tests/test_*.py` - unit tests (pytest)

```bash
./tests/test-stack.sh
./tests/test-services.sh
./tests/test-agent.sh
./tests/test-proxy-web-search-smoke.sh
python3 tests/test-tool-calling.py
python3 -m pytest tests/ -v
```

Override target model or URL: `MODEL=qwen3:8b ./tests/test-stack.sh`

## Teardown

```bash
./uninstall.sh           # keep persistent data
./uninstall.sh --purge   # remove everything including models and vectors
```

## Model Benchmark

`tests/bench-model-compare.sh` scores each model on when to call `web_search` vs answer directly. 20 calibrated prompts through the full agent stack.

| Model | Judgment | Search | NoSearch | JSON OK | E2E |
|-------|----------|--------|----------|---------|-----|
| devstral:latest | 20/20 | 12/12 | 8/8 | 20/20 | 1m 46s |
| qwen3:8b | 16/20 | 8/12 | 8/8 | 16/20 | 1m 03s |
| mistral-nemo:latest | 16/20 | 12/12 | 4/8 | 20/20 | 1m 19s |
| qwen2.5:14b | 15/20 | 9/12 | 6/8 | 17/20 | 14s |
| qwen3:14b | 13/20 | 5/12 | 8/8 | 13/20 | 1m 39s |
| llama3.1:8b | 10/20 | 10/12 | 0/8 | 18/20 | 54s |

- **Judgment** = Search + NoSearch combined score
- **Search** = correctly calls `web_search` on 12 current-info prompts
- **NoSearch** = correctly answers directly on 8 well-known-fact prompts
- **JSON OK** = valid tool call structure
- **E2E** = full stack latency

```bash
./tests/bench-model-compare.sh
```
