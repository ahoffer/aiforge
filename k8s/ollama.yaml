# ---------- Ollama (GPU) ----------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-data
  namespace: aiforge
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 200Gi
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-config
  namespace: aiforge
data:
  OLLAMA_HOST: "0.0.0.0:11434"
  OLLAMA_FLASH_ATTENTION: "1"
  OLLAMA_GPU_OVERHEAD: "33554432"
  OLLAMA_KEEP_ALIVE: "-1"
  OLLAMA_NUM_PARALLEL: "1"
  OLLAMA_MAX_LOADED_MODELS: "1"
  # Q4_0 KV cache at ~0.05 MiB per token. 32k context uses ~1.6 GB,
  # well within the ~5.7 GB free after qwen2.5-coder:14b model weights load.
  OLLAMA_KV_CACHE_TYPE: "q4_0"
  AGENT_MODEL: "qwen2.5-coder:14b"
  EMBEDDING_MODEL: "nomic-embed-text"
  # LLM response tuning. Applied by creating a model alias at startup.
  AGENT_TEMPERATURE: "0.3"
  AGENT_MAX_TOKENS: "2048"
  AGENT_REPEAT_PENALTY: "1.05"
  # Ollama defaults to 4096 context. Clients like Goose send large tool-heavy
  # prompts that get silently truncated. 32k fits comfortably with Q4_0 KV cache.
  AGENT_NUM_CTX: "32768"
  AGENT_SYSTEM_PROMPT: "You are a coding assistant. Read relevant files before making changes. After editing, verify your changes compile or pass linting. Use web_search for current events, real-time information, or whenever the user explicitly asks to search the web. Do not use web_search for well-known facts or concepts unless the user requests a search. For grep, find, sed, awk, or any text-processing task, use run_command. list_files only lists directory contents like ls. When the user asks for multiple actions, call all relevant tools in one response. Be concise and direct. Avoid filler phrases."
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-scripts
  namespace: aiforge
data:
  create-alias.sh: |
    #!/bin/sh
    # Generates a Modelfile from ConfigMap env vars and creates the
    # tuned agent alias. Shared by the postStart hook and warmup Job
    # so changes to alias creation happen in one place.
    set -eu
    ALIAS="${AGENT_MODEL}-agent"
    {
      echo "FROM ${AGENT_MODEL}"
      echo "PARAMETER temperature ${AGENT_TEMPERATURE}"
      echo "PARAMETER num_predict ${AGENT_MAX_TOKENS}"
      echo "PARAMETER repeat_penalty ${AGENT_REPEAT_PENALTY}"
      echo "PARAMETER num_ctx ${AGENT_NUM_CTX}"
      printf "SYSTEM %s\n" "${AGENT_SYSTEM_PROMPT}"
    } > /tmp/Modelfile
    if ! ollama show "$ALIAS" >/dev/null 2>&1; then
      ollama create "$ALIAS" -f /tmp/Modelfile
    fi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: aiforge
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      initContainers:
        - name: model-puller
          image: ollama/ollama:0.16.1
          command: ["/bin/sh", "-c"]
          args:
            - |
              ollama serve &
              sleep 5
              until ollama list >/dev/null 2>&1; do sleep 2; done
              # Pull base model for agent tool calling
              timeout 600 ollama pull "$AGENT_MODEL" || echo "Model pull failed or timed out"
              # Pull embedding model for RAG
              timeout 120 ollama pull "$EMBEDDING_MODEL" || echo "Embedding model pull failed or timed out"
          env:
            - name: AGENT_MODEL
              valueFrom:
                configMapKeyRef:
                  name: ollama-config
                  key: AGENT_MODEL
            - name: EMBEDDING_MODEL
              valueFrom:
                configMapKeyRef:
                  name: ollama-config
                  key: EMBEDDING_MODEL
          resources:
            requests:
              memory: 1Gi
              cpu: 500m
            limits:
              memory: 2Gi
              nvidia.com/gpu: 1
          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama
      containers:
        - name: ollama
          image: ollama/ollama:0.16.1
          ports:
            - containerPort: 11434
          envFrom:
            - configMapRef:
                name: ollama-config
          lifecycle:
            postStart:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - |
                    # Wait for local Ollama API to be ready before alias creation.
                    ready=false
                    i=1
                    while [ "$i" -le 30 ]; do
                      if ollama list >/dev/null 2>&1; then
                        ready=true
                        break
                      fi
                      sleep 2
                      i=$((i + 1))
                    done

                    if [ "$ready" != "true" ]; then
                      echo "postStart: Ollama API not ready, skipping alias creation"
                      exit 0
                    fi

                    /bin/sh /scripts/create-alias.sh || true
          resources:
            requests:
              memory: 2Gi
              cpu: 1000m
            limits:
              memory: 12Gi
              nvidia.com/gpu: 1
          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama
            - name: scripts
              mountPath: /scripts
          readinessProbe:
            httpGet:
              path: /
              port: 11434
            initialDelaySeconds: 10
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /
              port: 11434
            initialDelaySeconds: 30
            periodSeconds: 15
            timeoutSeconds: 5
            failureThreshold: 5
      volumes:
        - name: ollama-data
          persistentVolumeClaim:
            claimName: ollama-data
        - name: scripts
          configMap:
            name: ollama-scripts
            defaultMode: 0755
---
apiVersion: v1
kind: Service
metadata:
  name: ollama
  namespace: aiforge
spec:
  selector:
    app: ollama
  ports:
    - name: http
      port: 11434
      targetPort: 11434
  type: ClusterIP
