# ---------- Ollama (GPU) ----------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-data
  namespace: aiforge
spec:
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 200Gi
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-config
  namespace: aiforge
data:
  OLLAMA_HOST: "0.0.0.0:11434"
  OLLAMA_KEEP_ALIVE: "-1"
  OLLAMA_NUM_PARALLEL: "2"
  OLLAMA_MAX_LOADED_MODELS: "1"
  # Q4_0 KV cache quarters VRAM per token, fitting 32k context in ~2 GB.
  # Minor precision loss compared to Q8_0, large context gain for tool-heavy sessions.
  OLLAMA_KV_CACHE_TYPE: "q4_0"
  DEFAULT_MODEL: "devstral:latest"
  # LLM response tuning. Applied by creating a model alias at startup.
  AGENT_TEMPERATURE: "0.7"
  AGENT_MAX_TOKENS: "2048"
  AGENT_TOP_P: "0.9"
  AGENT_REPEAT_PENALTY: "1.2"
  # Feb 2025: Ollama defaults to 4096 context. Clients like Goose and opencode
  # send large tool-heavy prompts that get silently truncated. See ollama#5356.
  AGENT_NUM_CTX: "32768"
  AGENT_SYSTEM_PROMPT: "You are a coding assistant. Read relevant files before making changes. After editing, verify your changes compile or pass linting. Use web_search only when local context is insufficient, for example to check library versions or API changes beyond your training data. Be concise and direct. Avoid filler phrases."
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: aiforge
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: ollama
  template:
    metadata:
      labels:
        app: ollama
    spec:
      initContainers:
        - name: model-puller
          image: ollama/ollama:latest
          command: ["/bin/sh", "-c"]
          args:
            - |
              ollama serve &
              sleep 5
              until ollama list >/dev/null 2>&1; do sleep 2; done
              # Pull base model for agent tool calling
              timeout 600 ollama pull "$DEFAULT_MODEL" || echo "Model pull failed or timed out"
              # Pull embedding model for RAG
              timeout 120 ollama pull nomic-embed-text || echo "Embedding model pull failed or timed out"
          env:
            - name: DEFAULT_MODEL
              valueFrom:
                configMapKeyRef:
                  name: ollama-config
                  key: DEFAULT_MODEL
          resources:
            requests:
              memory: 1Gi
              cpu: 500m
            limits:
              memory: 2Gi
              nvidia.com/gpu: 1
          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama
      containers:
        - name: ollama
          image: ollama/ollama:latest
          ports:
            - containerPort: 11434
          envFrom:
            - configMapRef:
                name: ollama-config
          lifecycle:
            postStart:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - |
                    # Wait for local Ollama API to be ready before alias creation.
                    ready=false
                    i=1
                    while [ "$i" -le 30 ]; do
                      if ollama list >/dev/null 2>&1; then
                        ready=true
                        break
                      fi
                      sleep 2
                      i=$((i + 1))
                    done

                    if [ "$ready" != "true" ]; then
                      echo "postStart: Ollama API not ready, skipping alias creation"
                      exit 0
                    fi

                    # Create tuned model alias with verbosity controls from ConfigMap.
                    {
                      echo "FROM ${DEFAULT_MODEL}"
                      echo "PARAMETER temperature ${AGENT_TEMPERATURE}"
                      echo "PARAMETER num_predict ${AGENT_MAX_TOKENS}"
                      echo "PARAMETER top_p ${AGENT_TOP_P}"
                      echo "PARAMETER repeat_penalty ${AGENT_REPEAT_PENALTY}"
                      echo "PARAMETER num_ctx ${AGENT_NUM_CTX}"
                      printf "SYSTEM %s\n" "${AGENT_SYSTEM_PROMPT}"
                    } > /tmp/Modelfile

                    if ! ollama show "${DEFAULT_MODEL}-agent" >/dev/null 2>&1; then
                      ollama create "${DEFAULT_MODEL}-agent" -f /tmp/Modelfile || true
                    fi
          resources:
            requests:
              memory: 2Gi
              cpu: 1000m
            limits:
              memory: 12Gi
              nvidia.com/gpu: 1
          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama
          readinessProbe:
            httpGet:
              path: /
              port: 11434
            initialDelaySeconds: 10
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /
              port: 11434
            initialDelaySeconds: 30
            periodSeconds: 15
            timeoutSeconds: 5
            failureThreshold: 5
      volumes:
        - name: ollama-data
          persistentVolumeClaim:
            claimName: ollama-data
---
apiVersion: v1
kind: Service
metadata:
  name: ollama
  namespace: aiforge
spec:
  selector:
    app: ollama
  ports:
    - name: http
      port: 11434
      targetPort: 11434
  type: ClusterIP
