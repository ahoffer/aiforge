"""FastAPI entry point for Proteus, a smart proxy between LLM clients and Ollama.

Proteus intercepts web_search tool calls server-side via SearXNG while passing
all client-defined tool calls through transparently. A session store preserves
internal search context across request boundaries.
"""

import argparse
import asyncio
import hashlib
import json
import logging
import os
import sys
import time
from collections import OrderedDict
from contextlib import asynccontextmanager
from uuid import uuid4

import requests
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import uvicorn

from clients import OllamaClient, SearxngClient
from clients.ollama import _post_with_retry
from graph import agent_graph
from tools import WEB_SEARCH_TOOL

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(name)s %(levelname)s %(message)s",
    datefmt="%H:%M:%S",
)
log = logging.getLogger("proteus")

OLLAMA_URL = os.getenv("OLLAMA_URL", "http://ollama:11434")
AGENT_MODEL = os.getenv("AGENT_MODEL", "devstral:latest-agent")
MAX_PROXY_ITERATIONS = 5
PROXY_POLICY_MARKER = "[proteus_policy_v1]"
PROXY_OPENAI_SYSTEM_PROMPT = (
    f"{PROXY_POLICY_MARKER}\n"
    "You are running behind the Proteus proxy.\n"
    "Use the web_search tool for current, time-sensitive, or uncertain facts.\n"
    "For stable, well-known facts, answer directly without web_search.\n"
    "Do not claim you searched unless you actually called web_search."
)

SEARCH_CUE_WORDS = (
    "latest", "current", "today", "this year", "yesterday", "breaking", "recent",
    "release notes", "cve", "who won", "price", "stock", "version",
)
NO_SEARCH_CUE_WORDS = (
    "2+2", "2 + 2", "capital of france", "explain cap theorem",
    "difference between cmd and entrypoint", "python zip function",
)

# -- Native request/response models for Open WebUI --

class ChatRequest(BaseModel):
    """Request body for chat endpoint."""
    message: str
    conversation_id: str | None = None


class ChatResponse(BaseModel):
    """Response body for chat endpoint."""
    response: str
    sources: list[str] = []
    search_count: int = 0
    conversation_id: str


class HealthResponse(BaseModel):
    """Response body for health check."""
    status: str
    services: dict[str, bool]


# -- OpenAI-compatible models --

class OpenAIFunctionCall(BaseModel):
    name: str
    arguments: str  # JSON string per OpenAI spec


class OpenAIToolCall(BaseModel):
    id: str
    type: str = "function"
    function: OpenAIFunctionCall


class OpenAIMessage(BaseModel):
    role: str
    content: str | None = None
    tool_calls: list[OpenAIToolCall] | None = None
    tool_call_id: str | None = None
    name: str | None = None


class OpenAIChatRequest(BaseModel):
    model_config = {"extra": "allow"}
    model: str = "proteus"
    messages: list[OpenAIMessage] = []
    stream: bool = False
    tools: list[dict] | None = None
    tool_choice: str | dict | None = None
    temperature: float | None = None
    max_tokens: int | None = None
    top_p: float | None = None


# -- Session store --

SESSION_TTL = 1800  # 30 minutes
MAX_SESSIONS = 100


class SessionStore:
    """In-memory conversation state with LRU eviction and TTL.

    Internal messages generated by the proxy (web_search call/result pairs)
    are marked with _internal=True so they can be filtered when matching
    against client-provided history.
    """

    def __init__(self):
        self._sessions: OrderedDict[str, dict] = OrderedDict()

    def _fingerprint(self, messages: list[dict]) -> str:
        """Hash visible messages to identify a conversation."""
        parts = []
        for m in messages:
            parts.append(f"{m.get('role', '')}:{m.get('content', '')}")
        return hashlib.sha256("|".join(parts).encode()).hexdigest()[:16]

    def _visible(self, messages: list[dict]) -> list[dict]:
        """Filter out internal messages so we can match against client history."""
        return [m for m in messages if not m.get("_internal")]

    def _evict_expired(self):
        now = time.time()
        expired = [
            sid for sid, data in self._sessions.items()
            if now - data["ts"] > SESSION_TTL
        ]
        for sid in expired:
            del self._sessions[sid]

    def find(self, client_messages: list[dict]) -> tuple[str | None, list[dict]]:
        """Match client messages against stored sessions.

        Compares all client messages except the last (the new user input)
        against the visible messages in each stored session. If a match is
        found, returns the session id and the augmented history with the
        new user message appended.
        """
        self._evict_expired()

        if len(client_messages) <= 1:
            return None, client_messages

        prefix = client_messages[:-1]
        prefix_fp = self._fingerprint(prefix)

        for sid, data in reversed(self._sessions.items()):
            stored_visible = self._visible(data["messages"])
            stored_fp = self._fingerprint(stored_visible)

            if prefix_fp == stored_fp:
                # Append new user message to augmented history
                augmented = data["messages"] + [client_messages[-1]]
                self._sessions.move_to_end(sid)
                data["ts"] = time.time()
                return sid, augmented

        return None, client_messages

    def save(self, session_id: str, augmented_messages: list[dict]):
        """Store augmented history, evicting old sessions if needed."""
        self._sessions[session_id] = {
            "messages": augmented_messages,
            "ts": time.time(),
        }
        self._sessions.move_to_end(session_id)

        while len(self._sessions) > MAX_SESSIONS:
            self._sessions.popitem(last=False)


sessions = SessionStore()


# -- Proxy helpers --

def _merge_web_search(tools: list[dict] | None) -> list[dict]:
    """Add WEB_SEARCH_TOOL to the tool list if not already present."""
    tools = list(tools) if tools else []
    names = set()
    for t in tools:
        fn = t.get("function", {})
        names.add(fn.get("name", ""))
    if "web_search" not in names:
        tools.append(WEB_SEARCH_TOOL)
    return tools


def _search_intent_for_latest_user(messages: list[dict]) -> str:
    """Return search intent classification for the latest user message."""
    latest_user = ""
    for msg in reversed(messages):
        if msg.get("role") == "user":
            latest_user = str(msg.get("content", "")).lower()
            break

    if not latest_user:
        return "neutral"
    if any(cue in latest_user for cue in NO_SEARCH_CUE_WORDS):
        return "nosearch"
    if any(cue in latest_user for cue in SEARCH_CUE_WORDS):
        return "search"
    return "neutral"


def _ensure_proxy_system_prompt(messages: list[dict]) -> list[dict]:
    """Inject Proteus policy prompt once, with intent-specific guidance."""
    for msg in messages:
        if msg.get("role") == "system" and PROXY_POLICY_MARKER in str(msg.get("content", "")):
            return messages

    intent = _search_intent_for_latest_user(messages)
    intent_line = ""
    if intent == "search":
        intent_line = "\nThe latest user request appears time-sensitive: call web_search before final answer."
    elif intent == "nosearch":
        intent_line = "\nThe latest user request appears stable: answer directly unless truly uncertain."

    policy = f"{PROXY_OPENAI_SYSTEM_PROMPT}{intent_line}"

    if messages and messages[0].get("role") == "system":
        updated = list(messages)
        first = dict(updated[0])
        first_content = str(first.get("content", ""))
        first["content"] = f"{policy}\n\n{first_content}" if first_content else policy
        updated[0] = first
        return updated

    return [{"role": "system", "content": policy}, *messages]


def _has_internal_web_search_result(messages: list[dict]) -> bool:
    """True if proxy has already executed at least one internal web_search."""
    for msg in messages:
        if msg.get("role") == "tool" and msg.get("_internal"):
            return True
    return False


def _resolve_tool_choice(
    requested_tool_choice: str | dict | None,
    messages: list[dict],
    iteration: int,
    merged_tools: list[dict],
) -> str | dict | None:
    """Resolve tool_choice for an Ollama OpenAI-compatible call."""
    if requested_tool_choice is not None:
        return requested_tool_choice

    intent = _search_intent_for_latest_user(messages)
    has_search_tool = any(
        t.get("function", {}).get("name") == "web_search" for t in merged_tools
    )

    if (
        intent == "search"
        and has_search_tool
        and iteration == 0
        and not _has_internal_web_search_result(messages)
    ):
        return {
            "type": "function",
            "function": {"name": "web_search"},
        }

    return None


def _partition_tool_calls(calls: list[dict]) -> tuple[list[dict], list[dict]]:
    """Split tool calls into (web_search_calls, client_calls)."""
    web_calls = []
    client_calls = []
    for tc in calls:
        fn = tc.get("function", {})
        if fn.get("name") == "web_search":
            web_calls.append(tc)
        else:
            client_calls.append(tc)
    return web_calls, client_calls


def _execute_web_searches(web_calls: list[dict], searxng: SearxngClient) -> list[dict]:
    """Execute web_search tool calls and return tool response messages."""
    tool_messages = []
    for tc in web_calls:
        fn = tc.get("function", {})
        args_raw = fn.get("arguments", "{}")
        if isinstance(args_raw, str):
            try:
                args = json.loads(args_raw)
            except json.JSONDecodeError:
                args = {"query": args_raw}
        else:
            args = args_raw
        query = args.get("query", "")
        log.info("Proxy web_search: %s", query)
        search_result = searxng.search_text(query, max_results=5)
        tool_messages.append({
            "role": "tool",
            "tool_call_id": tc.get("id", f"call_{uuid4().hex[:8]}"),
            "content": search_result,
            "_internal": True,
        })
    return tool_messages


def _msg_to_dict(msg) -> dict:
    """Convert an OpenAIMessage (or dict) to a plain dict for Ollama."""
    if isinstance(msg, dict):
        return msg
    d = {"role": msg.role}
    if msg.content is not None:
        d["content"] = msg.content
    if msg.tool_calls:
        d["tool_calls"] = [
            {
                "id": tc.id,
                "type": tc.type,
                "function": {
                    "name": tc.function.name,
                    "arguments": tc.function.arguments,
                },
            }
            for tc in msg.tool_calls
        ]
    if msg.tool_call_id is not None:
        d["tool_call_id"] = msg.tool_call_id
    if msg.name is not None:
        d["name"] = msg.name
    return d


def _clean_for_ollama(messages: list[dict]) -> list[dict]:
    """Strip internal metadata before sending to Ollama."""
    cleaned = []
    for m in messages:
        mc = {k: v for k, v in m.items() if not k.startswith("_")}
        cleaned.append(mc)
    return cleaned


def _reconstruct_message_from_chunks(chunks: list[dict]) -> dict:
    """Merge streamed SSE chunks into a complete assistant message.

    OpenAI streaming sends tool_call deltas as incremental index-based
    fragments. This function reassembles them into a single message dict.
    """
    content_parts = []
    tool_calls_by_index: dict[int, dict] = {}

    for chunk in chunks:
        choices = chunk.get("choices", [])
        if not choices:
            continue
        delta = choices[0].get("delta", {})

        if "content" in delta and delta["content"]:
            content_parts.append(delta["content"])

        for tc_delta in delta.get("tool_calls", []):
            idx = tc_delta.get("index", 0)
            if idx not in tool_calls_by_index:
                tool_calls_by_index[idx] = {
                    "id": tc_delta.get("id", ""),
                    "type": tc_delta.get("type", "function"),
                    "function": {"name": "", "arguments": ""},
                }
            existing = tool_calls_by_index[idx]
            if tc_delta.get("id"):
                existing["id"] = tc_delta["id"]
            fn_delta = tc_delta.get("function", {})
            if fn_delta.get("name"):
                existing["function"]["name"] += fn_delta["name"]
            if fn_delta.get("arguments"):
                existing["function"]["arguments"] += fn_delta["arguments"]

    msg = {"role": "assistant"}
    content = "".join(content_parts)
    if content:
        msg["content"] = content
    if tool_calls_by_index:
        msg["tool_calls"] = [
            tool_calls_by_index[i] for i in sorted(tool_calls_by_index)
        ]
    return msg


# -- Application setup --

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan handler for startup and shutdown."""
    log.info("Proteus starting up")
    yield
    log.info("Proteus shutting down")


app = FastAPI(
    title="Proteus",
    description="Smart proxy with server-side web search orchestration",
    version="3.0.0",
    lifespan=lifespan,
)


# -- Health and native chat endpoints (unchanged for Open WebUI) --

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Check health of the agent and its dependencies."""
    ollama = OllamaClient()
    searxng = SearxngClient()

    services = {
        "ollama": ollama.health(),
        "searxng": searxng.health(),
    }

    overall = "healthy" if services["ollama"] else "degraded"

    return HealthResponse(status=overall, services=services)


@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """Process a chat message through the orchestrator graph."""
    log.info("POST /chat conversation_id=%s", request.conversation_id)
    conversation_id = request.conversation_id or str(uuid4())

    try:
        result = await asyncio.to_thread(agent_graph.invoke, {
            "message": request.message,
            "conversation_id": conversation_id,
        })

        return ChatResponse(
            response=result.get("final_response", ""),
            sources=result.get("sources", []),
            search_count=result.get("search_count", 0),
            conversation_id=conversation_id,
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """Stream chat response with progress updates via Server-Sent Events."""
    log.info("POST /chat/stream conversation_id=%s", request.conversation_id)
    conversation_id = request.conversation_id or str(uuid4())

    async def generate():
        try:
            graph_input = {
                "message": request.message,
                "conversation_id": conversation_id,
            }
            outputs = await asyncio.to_thread(
                lambda: list(agent_graph.stream(graph_input))
            )
            for output in outputs:
                for node_name, node_output in output.items():
                    yield f"event: node\ndata: {node_name}\n\n"

                    if "final_response" in node_output:
                        response = node_output["final_response"]
                        yield f"event: response\ndata: {response}\n\n"

            yield "event: done\ndata: complete\n\n"

        except Exception as e:
            yield f"event: error\ndata: {str(e)}\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        },
    )


# -- OpenAI-compatible proxy endpoints --

@app.get("/v1/models")
async def list_models():
    """Return model list for OpenAI client discovery."""
    return {
        "object": "list",
        "data": [
            {
                "id": "proteus",
                "object": "model",
                "created": 0,
                "owned_by": "local",
            }
        ],
    }


@app.post("/v1/chat/completions")
async def openai_chat_completions(request: OpenAIChatRequest):
    """OpenAI-compatible chat completions proxy.

    Forwards requests to Ollama, intercepts web_search tool calls
    server-side, and passes client tool calls through transparently.
    """
    if not request.messages:
        raise HTTPException(status_code=400, detail="No messages provided")

    log.info("POST /v1/chat/completions messages=%d stream=%s",
             len(request.messages), request.stream)

    if request.stream:
        return await _proxy_streaming(request)

    return await _proxy_non_streaming(request)


async def _proxy_non_streaming(request: OpenAIChatRequest):
    """Proxy loop for non-streaming completions."""
    searxng = SearxngClient()
    completion_id = f"chatcmpl-{uuid4().hex[:12]}"
    created = int(time.time())

    client_messages = [_msg_to_dict(m) for m in request.messages]
    merged_tools = _merge_web_search(request.tools)

    # Restore session context if available
    session_id, messages = sessions.find(client_messages)
    if session_id is None:
        session_id = uuid4().hex[:16]
        messages = client_messages
    messages = _ensure_proxy_system_prompt(messages)

    for iteration in range(MAX_PROXY_ITERATIONS):
        payload = {
            "model": AGENT_MODEL,
            "messages": _clean_for_ollama(messages),
            "stream": False,
            "tools": merged_tools,
        }
        tool_choice = _resolve_tool_choice(request.tool_choice, messages, iteration, merged_tools)
        if tool_choice is not None:
            payload["tool_choice"] = tool_choice
        if request.temperature is not None:
            payload["temperature"] = request.temperature
        if request.max_tokens is not None:
            payload["max_tokens"] = request.max_tokens
        if request.top_p is not None:
            payload["top_p"] = request.top_p

        try:
            t0 = time.time()
            resp = await asyncio.to_thread(
                _post_with_retry,
                f"{OLLAMA_URL}/v1/chat/completions",
                payload,
                timeout=300,
            )
            resp.raise_for_status()
            resp_json = resp.json()
            elapsed = time.time() - t0
        except Exception as e:
            log.error("Ollama request failed: %s", e)
            raise HTTPException(status_code=502, detail=f"Ollama error: {e}")

        choice = resp_json.get("choices", [{}])[0]
        assistant_msg = choice.get("message", {})
        finish_reason = choice.get("finish_reason", "stop")
        tool_calls = assistant_msg.get("tool_calls", [])

        tool_names = [tc.get("function", {}).get("name", "?") for tc in tool_calls]
        log.info("Iteration %d ollama_time=%.1fs tool_calls=%s finish=%s",
                 iteration + 1, elapsed, tool_names or "none", finish_reason)

        messages.append(assistant_msg)

        if not tool_calls:
            sessions.save(session_id, messages)
            return resp_json

        web_calls, client_calls = _partition_tool_calls(tool_calls)

        if web_calls:
            if not client_calls:
                assistant_msg["_internal"] = True
            t_search = time.time()
            tool_responses = await asyncio.to_thread(
                _execute_web_searches, web_calls, searxng
            )
            search_elapsed = time.time() - t_search
            messages.extend(tool_responses)
            log.info("Iteration %d web_search count=%d search_time=%.1fs",
                     iteration + 1, len(web_calls), search_elapsed)

            if client_calls:
                continue
            continue

        sessions.save(session_id, messages)
        return resp_json

    sessions.save(session_id, messages)
    return resp_json


async def _proxy_streaming(request: OpenAIChatRequest):
    """Proxy loop for streaming completions."""
    searxng = SearxngClient()
    completion_id = f"chatcmpl-{uuid4().hex[:12]}"
    created = int(time.time())

    client_messages = [_msg_to_dict(m) for m in request.messages]
    merged_tools = _merge_web_search(request.tools)

    session_id, messages = sessions.find(client_messages)
    if session_id is None:
        session_id = uuid4().hex[:16]
        messages = client_messages
    messages = _ensure_proxy_system_prompt(messages)

    async def generate():
        nonlocal messages

        for iteration in range(MAX_PROXY_ITERATIONS):
            payload = {
                "model": AGENT_MODEL,
                "messages": _clean_for_ollama(messages),
                "stream": True,
                "tools": merged_tools,
            }
            tool_choice = _resolve_tool_choice(request.tool_choice, messages, iteration, merged_tools)
            if tool_choice is not None:
                payload["tool_choice"] = tool_choice
            if request.temperature is not None:
                payload["temperature"] = request.temperature
            if request.max_tokens is not None:
                payload["max_tokens"] = request.max_tokens
            if request.top_p is not None:
                payload["top_p"] = request.top_p

            try:
                t0 = time.time()
                resp = await asyncio.to_thread(
                    lambda: requests.post(
                        f"{OLLAMA_URL}/v1/chat/completions",
                        json=payload,
                        stream=True,
                        timeout=300,
                    )
                )
                resp.raise_for_status()
            except Exception as e:
                log.error("Ollama streaming request failed: %s", e)
                yield f"data: {json.dumps({'error': str(e)})}\n\n"
                yield "data: [DONE]\n\n"
                return

            accumulated_chunks = []
            raw_lines = []

            for line in resp.iter_lines():
                if not line:
                    continue
                text = line.decode("utf-8") if isinstance(line, bytes) else line
                if not text.startswith("data: "):
                    continue
                data_str = text[6:]
                if data_str.strip() == "[DONE]":
                    raw_lines.append(text)
                    continue
                try:
                    chunk = json.loads(data_str)
                    accumulated_chunks.append(chunk)
                    raw_lines.append(text)
                except json.JSONDecodeError:
                    raw_lines.append(text)

            elapsed = time.time() - t0
            assistant_msg = _reconstruct_message_from_chunks(accumulated_chunks)
            tool_calls = assistant_msg.get("tool_calls", [])

            tool_names = [tc.get("function", {}).get("name", "?") for tc in tool_calls]
            log.info("Stream iteration %d ollama_time=%.1fs tool_calls=%s",
                     iteration + 1, elapsed, tool_names or "none")

            if not tool_calls:
                messages.append(assistant_msg)
                sessions.save(session_id, messages)
                for raw_line in raw_lines:
                    yield f"{raw_line}\n\n"
                return

            web_calls, client_calls = _partition_tool_calls(tool_calls)

            if web_calls:
                if not client_calls:
                    assistant_msg["_internal"] = True
                messages.append(assistant_msg)
                t_search = time.time()
                tool_responses = _execute_web_searches(web_calls, searxng)
                search_elapsed = time.time() - t_search
                messages.extend(tool_responses)
                log.info("Stream iteration %d web_search count=%d search_time=%.1fs",
                         iteration + 1, len(web_calls), search_elapsed)

                if client_calls:
                    continue
                continue

            messages.append(assistant_msg)
            sessions.save(session_id, messages)
            for raw_line in raw_lines:
                yield f"{raw_line}\n\n"
            return

        # Max iterations, return last accumulated response as non-streamed
        sessions.save(session_id, messages)
        fallback = {
            "id": completion_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": "proteus",
            "choices": [{
                "index": 0,
                "delta": {"content": assistant_msg.get("content", "")},
                "finish_reason": None,
            }],
        }
        yield f"data: {json.dumps(fallback)}\n\n"
        done = {
            "id": completion_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": "proteus",
            "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
        }
        yield f"data: {json.dumps(done)}\n\n"
        yield "data: [DONE]\n\n"

    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        },
    )


def main():
    """Entry point for CLI."""
    parser = argparse.ArgumentParser(description="Proteus")
    parser.add_argument(
        "--serve",
        action="store_true",
        help="Start the HTTP server",
    )
    parser.add_argument(
        "--host",
        default="0.0.0.0",
        help="Host to bind to (default: 0.0.0.0)",
    )
    parser.add_argument(
        "--port",
        type=int,
        default=8000,
        help="Port to bind to (default: 8000)",
    )
    parser.add_argument(
        "message",
        nargs="*",
        help="Message to process (CLI mode)",
    )

    args = parser.parse_args()

    if args.serve:
        uvicorn.run(app, host=args.host, port=args.port)
    elif args.message:
        message = " ".join(args.message)
        result = agent_graph.invoke({"message": message})
        print(result.get("final_response", "No response"))
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()
